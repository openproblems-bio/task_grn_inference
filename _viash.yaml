viash_version: 0.9.1
name: task_grn_inference
organization: openproblems-bio
version: build_main
license: MIT
keywords: [gene regulatory network, network inference]
links:
  issue_tracker: https://github.com/openproblems-bio/task_grn_inference/issues
  repository: https://github.com/openproblems-bio/task_grn_inference
  docker_registry: ghcr.io
label: GRN Inference
summary: "Benchmarking GRN inference methods\nLeaderboard: \n[Performance comparision](https://add-grn--openproblems.netlify.app/results/grn_inference/)\n\nArticle: [geneRNIB: a living benchmark for gene regulatory network inference](https://www.biorxiv.org/content/10.1101/2025.02.25.640181v1)\n\nDocumentation: \n[geneRNBI-doc](https://genernib-documentation.readthedocs.io/en/latest/)\n\nRepository:\n[openproblems-bio/task_grn_inference](https://github.com/openproblems-bio/task_grn_inference)\n\nIf you use this framework, please cite it as\n@article{nourisa2025genernib,\n  title={geneRNIB: a living benchmark for gene regulatory network inference},\n  author={Nourisa, Jalil and Passemiers, Antoine and Stock, Marco and Zeller-Plumhoff, Berit and Cannoodt, Robrecht and Arnold, Christian and Tong, Alexander and Hartford, Jason and Scialdone, Antonio and Moreau, Yves and others},\n  journal={bioRxiv},\n  pages={2025--02},\n  year={2025},\n  publisher={Cold Spring Harbor Laboratory}\n}\n"
description: "\ngeneRNIB is a living benchmark platform for GRN inference. This platform provides curated datasets for GRN inference and evaluation, standardized evaluation protocols and metrics, computational infrastructure, and a dynamically updated leaderboard to track state-of-the-art methods. It runs novel GRNs in the cloud, offers competition scores, and stores them for future comparisons, reflecting new developments over time.\n\nThe platform supports the integration of new inference methods, datasets and protocols. When a new feature is added, previously evaluated GRNs are re-assessed, and the leaderboard is updated accordingly. The aim is to evaluate both the accuracy and completeness of inferred GRNs. It is designed for both single-modality and multi-omics GRN inference. \n\nIn the current version, geneRNIB contains 10 inference methods including both single and multi-omics, 8 evalation metrics, and five datasets. \n\nSee our publication for the details of methods. \n"
info:
  image: thumbnail.svg # todo: create a thumbnail
  test_resources:
    - type: s3
      path: s3://openproblems-data/resources_test/grn
      dest: resources_test
  readme: "## Installation\n\nYou need to have Docker, Java, and Viash installed. Follow\n[these instructions](https://openproblems.bio/documentation/fundamentals/requirements)\nto install the required dependencies. \n\n## Download resources\n```bash\ngit clone git@github.com:openproblems-bio/task_grn_inference.git\n\ncd task_grn_inference\n```\nTo interact with the framework, you should download the resources containing necessary inferene and evaluation datasets to get started. \nHere, we download the **test resources** which are solely used for testing if the framework is installed successfully. \n\n```bash\nscripts/download_resources.sh\n```\n\nRefer to the [Documentation](https://genernib-documentation.readthedocs.io/en/latest/) for downloading the actual datasets. To reproduce the results, run `scripts/run_benchmark_all.sh`, which is a very resource intensive run.\n\n## Run a GRN inference method \n\nTo infer a GRN for a given dataset (e.g. `op`) using simple Pearson correlation:\n\n```bash\nviash run src/control_methods/pearson_corr/config.vsh.yaml -- \\\n      --rna resources_test/grn_benchmark/inference_data/op_rna.h5ad \\\n      --prediction output/net.h5ad \\\n      --tf_all resources_test/grn_benchmark/prior/tf_all.csv\n```\nIt should be noted that this is using the `resources_test` datasets, which are small versions of the actual datasets. Thus, the obtained predictions are not realistic. To obtain a realistic prediction, download the actual data and set the folder to `resources`.  \n\n## Evaluate a GRN prediction\nOnce got the prediction for a given dataset (e.g. op), use the following code to obtain evaluation scores. \n\n```bash\nscripts/single_grn_evaluation.sh output/net.h5ad op\n```\n\nThis outputs the scores into `output/test_run/scores.yaml`\n\n## Add a GRN inference method, evaluation metric, or dataset\n\nTo add a new component to the repository, follow the [Documentation](https://genernib-documentation.readthedocs.io/en/latest/).\n"
authors:
  - name: Jalil Nourisa
    roles: [author]
    info:
      github: janursa
      orcid: 0000-0002-7539-4396
  - name: Robrecht Cannoodt
    roles: [author]
    info:
      github: rcannood
      orcid: "0000-0003-3641-729X"
  - name: Antoine Passimier
    roles: [contributor]
    info:
      github: AntoinePassemiers
  - name: Marco Stock
    roles: [contributor]
    info:
      github: stkmrc
  - name: Christian Arnold
    roles: [contributor]
    info:
      github: chrarnold
config_mods: |
  .runners[.type == "nextflow"].config.labels := { lowmem : "memory = 20.Gb", midmem : "memory = 50.Gb", highmem : "memory = 100.Gb",  veryhighmemory : "memory = 200.Gb", veryveryhighmemory : "memory = 300.Gb", lowcpu : "cpus = 5", midcpu : "cpus = 15", highcpu : "cpus = 30", lowtime : "time = 1.h", midtime : "time = 4.h", hightime : "time = 8.h", veryhightime : "time = 24.h", twodaytime : "time = 28.h" }
repositories:
  - name: openproblems
    type: github
    repo: openproblems-bio/openproblems
    tag: build/main
