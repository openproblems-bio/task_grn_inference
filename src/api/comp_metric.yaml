functionality:
  namespace: "metrics"
  info:
    type: metrics
    type_info:
      label: Label
      summary: A metric to evaluate the performance of the inferred GRN
      description: |
        A metric to evaluate the performance of the inferred GRN
  arguments:
    - name: --perturbation_data
      __merge__: file_perturbation_h5ad.yaml
      required: false
      direction: input
      default: resources/grn-benchmark/perturbation_data.h5ad
    - name: --prediction
      __merge__: file_prediction.yaml
      required: false
      direction: input
      default: resources/grn_models/collectri.csv
    - name: --score
      __merge__: file_score.yaml
      required: false
      direction: output
    - name: --reg_type
      type: string
      direction: input
      default: ridge
      description: name of regretion to use
      multiple: false
      info:
        test_default: ridge
    - name: --layer
      type: string
      direction: input
      required: false
      default: pearson
    - name: --subsample
      type: integer
      direction: input
      default: -1
      description: number of samples randomly drawn from perturbation data
      info:
        test_default: 200
    - name: --max_workers
      type: integer
      direction: input
      default: 4


      
  test_resources:
    - type: python_script
      path: /src/common/component_tests/run_and_check_output.py
    - path: /resources/grn-benchmark
      dest: resources/grn-benchmark