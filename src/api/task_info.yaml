name: GRN inference benchmark
label: Living benchmark for gene regulatory network (GRN) inference  
motivation: |
  GRNs are essential for understanding cellular identity and behavior. They are simplified models of gene expression regulated by complex processes involving multiple layers of control, from transcription to post-transcriptional modifications, incorporating various regulatory elements and non-coding RNAs. Gene transcription is controlled by a regulatory complex that includes transcription factors (TFs), cis-regulatory elements (CREs) like promoters and enhancers, and essential co-factors. High-throughput datasets, covering thousands of genes, facilitate the use of machine learning approaches to decipher GRNs. The advent of single-cell sequencing technologies, such as scRNA-seq, has made it possible to infer GRNs from a single experiment due to the abundance of samples. This allows researchers to infer condition-specific GRNs, such as for different cell types or diseases, and study potential regulatory factors associated with these conditions. Combining chromatin accessibility data with gene expression measurements has led to the development of enhancer-driven GRN (eGRN) inference pipelines, which offer significantly improved accuracy over single-modality methods.
description: |
  Here, we present geneRNIB as a living benchmark platform for GRN inference. This platform provides curated datasets for GRN inference and evaluation, standardized evaluation protocols and metrics, computational infrastructure, and a dynamically updated leaderboard to track state-of-the-art methods. It runs novel GRNs in the cloud, offers competition scores, and stores them for future comparisons, reflecting new developments over time.

  The platform supports the integration of new datasets and protocols. When a new feature is added, previously evaluated GRNs are re-assessed, and the leaderboard is updated accordingly. The aim is to evaluate both the accuracy and completeness of inferred GRNs. It is designed for both single-modality and multi-omics GRN inference. Ultimately, it is a community-driven platform. 
  
  So far, ten GRN inference methods have been integrated: five sinlge-omics methods of GRNBoost2, GENIE3, Portia, PPCOR, and Scenic; and five eGRN inference methods of Scenic+, CellOracle, FigR, scGLUE, and GRaNIE.

  Due to its flexible nature, the platform can incorporate various benchmark datasets and evaluation methods, using either prior knowledge or feature-based approaches. 
  In the current version, due to the absence of standardized prior knowledge, we use indirect approaches to benchmark GRNs. Employing interventional data as evaluation datasets, we have developed 8 metrics using feature-based approach and Wasserstein distance, accounting for both accuracy and comprehensiveness.

  Five datasets have been integrated so far, namely OPSCA, Nakatake, Norman, Adamson, and Replogle. For each dataset, standardized inference datasets are provided to be used for GRN inference and evaluation datasets are employed to benchmark.
  See our publication for the details of methods. 


summary: |
  Benchmarking GRN inference methods
  The full documentation is hosted on [ReadTheDocs](https://grn-inference-benchmarking.readthedocs.io/en/latest/index.html). 
readme: |
  ## Installation

  You need to have Docker, Java, and Viash installed. Follow
  [these instructions](https://openproblems.bio/documentation/fundamentals/requirements)
  to install the required dependencies. 

  ## Download resources
  ```bash
  git clone git@github.com:openproblems-bio/task_grn_inference.git

  cd task_grn_inference
  
  # download resources
  scripts/download_resources.sh
  ```
  The datasets for GRN inference are located in `resources/inference_datasets`. 
  ## Infer a GRN 
  One GRN should be inferred for each inference dataset (op, norman, replogle2, adamson, and nakatake). The inferred GRN should have three columns of `source, target, weight`. See `resources/grn_models/op/grnboost2.csv` as an example.
  
  ## Evaluate a GRN
  Once a GRN is inferred (e.g. located in `output/your_GRN.csv`) for a given dataset (e.g. `norman`), use the following code to obtain evaluation scores. 
  ```bash
  scripts/calculate_score.sh output/your_GRN.csv norman
  ```
  This will calculate and print the scores as well as output the scores into `output/score.h5ad`
 
  ## Add a method

  To add a method to the repository, follow the instructions in the `scripts/add_a_method.sh` script.

  
authors:
  - name: Jalil Nourisa
    roles: [ author ]
    info:
      github: janursa
      orcid: 0000-0002-7539-4396
  - name: Robrecht Cannoodt
    roles: [ author ]
    info:
      github: rcannood
      orcid: "0000-0003-3641-729X"
  - name: Antoine Passimier 
    roles: [ contributor ]
    info:
      github: AntoinePassemiers
  - name: Marco Stock
    roles: [ contributor ]
    info:
      github: stkmrc
  - name: Christian Arnold 
    roles: [ contributor ]
    info:
      github: chrarnold




